#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒ V2 - å¢å¼ºç‰ˆ
- ä½¿ç”¨V2ç‰¹å¾ï¼ˆ18ç»´ï¼‰
- æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- å¤šé˜ˆå€¼å›æµ‹
"""
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle

try:
    from xgboost import XGBRegressor
    HAS_XGB = True
except ImportError:
    from sklearn.linear_model import LinearRegression
    HAS_XGB = False
    print("âš ï¸  XGBoostæœªå®‰è£…ï¼Œä½¿ç”¨LinearRegression")

PROJECT_ROOT = Path(__file__).parent.parent
FEATURES_DIR = PROJECT_ROOT / 'data' / 'features'
MODELS_DIR = PROJECT_ROOT / 'models'
MODELS_DIR.mkdir(parents=True, exist_ok=True)

def load_features():
    """åŠ è½½V2ç‰¹å¾"""
    filepath = FEATURES_DIR / 'features_v3.csv'
    df = pd.read_csv(filepath)
    print(f"ğŸ“Š åŠ è½½äº† {len(df)} åœºæ¯”èµ›ç‰¹å¾ï¼ˆV2å¢å¼ºç‰ˆï¼‰")
    return df

def prepare_data(df):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print(f"\nğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæ—¶é—´åºåˆ—é‡è¦ï¼ï¼‰
    df = df.sort_values('game_date').copy()
    
    # åˆ é™¤ç¼ºå¤±å€¼
    df = df.dropna()
    print(f"   åˆ é™¤ç¼ºå¤±å€¼å: {len(df)} åœº")
    
    # ç‰¹å¾åˆ—ï¼ˆæ’é™¤å…ƒæ•°æ®å’Œæ ‡ç­¾ï¼‰
    exclude_cols = ['game_id', 'game_date', 'home_team', 'away_team', 
                    'total_points', 'home_points', 'away_points']
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    
    X = df[feature_cols]
    y = df['total_points']
    
    print(f"   ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"   ç›®æ ‡èŒƒå›´: {y.min():.0f} - {y.max():.0f}, å‡å€¼: {y.mean():.1f}")
    print(f"   ä½¿ç”¨ç‰¹å¾: {len(feature_cols)} ä¸ª")
    
    return X, y, feature_cols, df['game_date']

def time_series_cv(X, y, dates, n_splits=5):
    """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
    print(f"\nğŸ”„ æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ ({n_splits} æŠ˜)...")
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        if HAS_XGB:
            model = XGBRegressor(n_estimators=100, learning_rate=0.05, 
                                max_depth=4, random_state=42)
        else:
            model = LinearRegression()
        
        model.fit(X_train, y_train)
        val_pred = model.predict(X_val)
        
        mae = mean_absolute_error(y_val, val_pred)
        cv_scores.append(mae)
        
        val_dates = dates.iloc[val_idx]
        print(f"   Fold {fold}: MAE={mae:.2f} (éªŒè¯é›†: {val_dates.min()} ~ {val_dates.max()})")
    
    print(f"\n   å¹³å‡MAE: {np.mean(cv_scores):.2f} Â± {np.std(cv_scores):.2f}")
    
    return cv_scores

def train_final_model(X, y):
    """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    print(f"\nğŸ¤– è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    
    # 80/20 æ—¶é—´åºåˆ—åˆ†å‰²
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} åœº")
    print(f"   æµ‹è¯•é›†: {len(X_test)} åœº")
    
    if HAS_XGB:
        model = XGBRegressor(
            n_estimators=150,
            learning_rate=0.05,
            max_depth=5,
            min_child_weight=3,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        model_name = 'XGBoost'
    else:
        model = LinearRegression()
        model_name = 'LinearRegression'
    
    print(f"   æ¨¡å‹: {model_name}")
    model.fit(X_train, y_train)
    
    # è¯„ä¼°
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_mae = mean_absolute_error(y_train, train_pred)
    test_mae = mean_absolute_error(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    test_r2 = r2_score(y_test, test_pred)
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½:")
    print(f"   è®­ç»ƒMAE: {train_mae:.2f} åˆ†")
    print(f"   æµ‹è¯•MAE: {test_mae:.2f} åˆ†")
    print(f"   æµ‹è¯•RMSE: {test_rmse:.2f} åˆ†")
    print(f"   æµ‹è¯•RÂ²: {test_r2:.3f}")
    
    return model, X_test, y_test, test_pred

def analyze_feature_importance(model, feature_cols):
    """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
    if not HAS_XGB:
        print("\nâš ï¸  LinearRegressionä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        return
    
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§ TOP 10:")
    
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:30s} {row['importance']:.4f}")
    
    return feature_importance

def evaluate_betting_strategy(y_true, y_pred, lines=[215, 220, 225, 230]):
    """å¤šç›˜å£çº¿å›æµ‹"""
    print(f"\nğŸ¯ åšå½©ç­–ç•¥å›æµ‹ (å¤šç›˜å£çº¿)...\n")
    
    results = []
    
    for line in lines:
        pred_over = y_pred > line
        actual_over = y_true > line
        
        correct = (pred_over == actual_over).sum()
        total = len(y_true)
        accuracy = correct / total * 100
        
        wins = correct
        losses = total - correct
        roi = (wins * 0.91 - losses) / total * 100
        
        results.append({
            'line': line,
            'accuracy': accuracy,
            'wins': wins,
            'losses': losses,
            'roi': roi
        })
        
        status = "âœ…" if accuracy > 52.4 else "âŒ"
        print(f"   ç›˜å£ {line}: {accuracy:.1f}% ({wins}èƒœ/{losses}è´Ÿ) ROI: {roi:+.1f}% {status}")
    
    # æ‰¾æœ€ä½³ç›˜å£
    best = max(results, key=lambda x: x['roi'])
    print(f"\n   ğŸ† æœ€ä½³ç›˜å£: {best['line']} (ROI {best['roi']:+.1f}%, å‡†ç¡®ç‡ {best['accuracy']:.1f}%)")
    
    return results

def save_model(model, feature_cols, cv_scores):
    """ä¿å­˜æ¨¡å‹"""
    filepath = MODELS_DIR / 'total_points_model_v3.pkl'
    
    model_package = {
        'model': model,
        'feature_cols': feature_cols,
        'cv_scores': cv_scores,
        'version': '2.0',
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    with open(filepath, 'wb') as f:
        pickle.dump(model_package, f)
    
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
    print(f"   å¤§å°: {filepath.stat().st_size / 1024:.1f} KB")

def main():
    print("\n" + "="*70)
    print("ğŸ¤– NBAå¤§å°åˆ†é¢„æµ‹æ¨¡å‹è®­ç»ƒ V3 (é›†æˆä¼¤ç—…)")
    print("="*70 + "\n")
    
    # åŠ è½½ç‰¹å¾
    df = load_features()
    
    # å‡†å¤‡æ•°æ®
    X, y, feature_cols, dates = prepare_data(df)
    
    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
    cv_scores = time_series_cv(X, y, dates, n_splits=5)
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model, X_test, y_test, test_pred = train_final_model(X, y)
    
    # ç‰¹å¾é‡è¦æ€§
    analyze_feature_importance(model, feature_cols)
    
    # åšå½©ç­–ç•¥è¯„ä¼°
    evaluate_betting_strategy(y_test.values, test_pred)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, feature_cols, cv_scores)
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ")
    print("="*70 + "\n")

if __name__ == '__main__':
    main()
