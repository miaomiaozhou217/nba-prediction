#!/usr/bin/env python3
"""
è®­ç»ƒ V4 æ¨¡å‹ (24ç»´ç‰¹å¾)
å¯¹æ¯”V3æ¨¡å‹ (20ç»´) éªŒè¯é˜²å®ˆ+èŠ‚å¥ç‰¹å¾çš„æ•ˆæœ

è¯„ä¼°æŒ‡æ ‡:
- RMSE (è¶Šä½è¶Šå¥½)
- MAE (å¹³å‡ç»å¯¹è¯¯å·®, ç›®æ ‡: <15åˆ†)
- RÂ² (æ‹Ÿåˆä¼˜åº¦, è¶Šæ¥è¿‘1è¶Šå¥½)
- Line 215 Accuracy (æ€»åˆ†é¢„æµ‹å‡†ç¡®åº¦)
"""
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import pickle

PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / 'data'
FEATURES_DIR = DATA_DIR / 'features'
MODELS_DIR = PROJECT_ROOT / 'models'
MODELS_DIR.mkdir(parents=True, exist_ok=True)

def load_features():
    """åŠ è½½V4ç‰¹å¾æ•°æ®"""
    filepath = FEATURES_DIR / 'features_v4.csv'
    df = pd.read_csv(filepath)
    print(f"ğŸ“Š åŠ è½½äº† {len(df)} åœºæ¯”èµ›çš„ç‰¹å¾ (V4: 24ç»´)")
    return df

def prepare_data(df):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    # åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è¡Œ
    df = df.dropna(subset=['combined_pts_last_3', 'combined_pts_last_5'])
    
    # ç‰¹å¾åˆ— (V4: 24ä¸ª)
    feature_cols = [
        # V2åŸºç¡€ (18ä¸ª)
        'home_pts_last_3', 'home_pts_last_5', 'home_pts_last_10',
        'home_opp_pts_last_5', 'home_pts_std_5', 'home_pts_last_5_home',
        'away_pts_last_3', 'away_pts_last_5', 'away_pts_last_10',
        'away_opp_pts_last_5', 'away_pts_std_5', 'away_pts_last_5_away',
        'combined_pts_last_3', 'combined_pts_last_5', 'combined_pts_last_10',
        'home_off_vs_away_def', 'away_off_vs_home_def', 'home_field_advantage',
        # V3ä¼¤ç—… (2ä¸ª)
        'home_injury_impact', 'away_injury_impact',
        # V4é˜²å®ˆèŠ‚å¥ (4ä¸ª)
        'home_def_rating_last_10', 'away_def_rating_last_10',
        'home_pace_last_10', 'away_pace_last_10'
    ]
    
    X = df[feature_cols].fillna(0)
    y = df['total_points']
    
    # ä¿ç•™å…ƒæ•°æ®ç”¨äºè¯„ä¼°
    metadata = df[['game_id', 'game_date', 'home_team', 'away_team', 'total_points']].copy()
    
    print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(X)} åœº")
    print(f"   ç‰¹å¾ç»´åº¦: {len(feature_cols)} ç»´")
    print(f"   - V2åŸºç¡€: 18ç»´")
    print(f"   - V3ä¼¤ç—…: 2ç»´")
    print(f"   - V4é˜²å®ˆèŠ‚å¥: 4ç»´")
    
    return X, y, metadata, feature_cols

def train_model(X, y, metadata):
    """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è®­ç»ƒ + çœŸæ­£çš„out-of-sampleæµ‹è¯•"""
    print(f"\nğŸ”§ è®­ç»ƒ XGBoost æ¨¡å‹ (5æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯)...\n")
    
    tscv = TimeSeriesSplit(n_splits=5)
    fold_results = []
    all_predictions = []  # æ”¶é›†æ‰€æœ‰foldçš„é¢„æµ‹
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = xgb.XGBRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            min_child_weight=3,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train, verbose=False)
        y_pred = model.predict(X_val)
        
        rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        mae = mean_absolute_error(y_val, y_pred)
        r2 = r2_score(y_val, y_pred)
        
        print(f"   Fold {fold}: RMSE={rmse:.2f}, MAE={mae:.2f}, RÂ²={r2:.3f}")
        fold_results.append({'rmse': rmse, 'mae': mae, 'r2': r2})
        
        # ä¿å­˜éªŒè¯é›†é¢„æµ‹ï¼ˆç”¨äºout-of-sampleè¯„ä¼°ï¼‰
        for idx, pred in zip(val_idx, y_pred):
            all_predictions.append({
                'index': idx,
                'actual': y.iloc[idx],
                'predicted': pred,
                'game_id': metadata.iloc[idx]['game_id'],
                'game_date': metadata.iloc[idx]['game_date'],
                'home_team': metadata.iloc[idx]['home_team'],
                'away_team': metadata.iloc[idx]['away_team']
            })
    
    # æ±‡æ€»CVç»“æœ
    avg_rmse = np.mean([r['rmse'] for r in fold_results])
    avg_mae = np.mean([r['mae'] for r in fold_results])
    avg_r2 = np.mean([r['r2'] for r in fold_results])
    
    print(f"\nğŸ“Š äº¤å‰éªŒè¯å¹³å‡ç»“æœ:")
    print(f"   RMSE: {avg_rmse:.2f} åˆ†")
    print(f"   MAE:  {avg_mae:.2f} åˆ†")
    print(f"   RÂ²:   {avg_r2:.3f}")
    
    # ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ”§ ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    final_model = xgb.XGBRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=6,
        min_child_weight=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    final_model.fit(X, y, verbose=False)
    
    return final_model, {'avg_rmse': avg_rmse, 'avg_mae': avg_mae, 'avg_r2': avg_r2}, all_predictions

def evaluate_line_accuracy(all_predictions):
    """è¯„ä¼°ç›˜å£å‡†ç¡®ç‡ (Line 215) - ä½¿ç”¨CVçš„out-of-sampleé¢„æµ‹"""
    print(f"\nğŸ¯ è¯„ä¼°ç›˜å£å‡†ç¡®ç‡ (Line 215, Out-of-Sample)...")
    
    # ä½¿ç”¨CVæœŸé—´çš„éªŒè¯é›†é¢„æµ‹ï¼ˆçœŸæ­£çš„out-of-sampleï¼‰
    eval_df = pd.DataFrame(all_predictions)
    eval_df['total_points'] = eval_df['actual']
    eval_df['error'] = eval_df['total_points'] - eval_df['predicted']
    
    # Line 215 å‡†ç¡®ç‡
    correct_215 = sum((eval_df['total_points'] > 215) == (eval_df['predicted'] > 215))
    accuracy_215 = correct_215 / len(eval_df) * 100
    
    # æŠ•æ³¨æ¨¡æ‹Ÿ (åªæŠ¼>5%ä¿¡å¿ƒçš„)
    eval_df['confidence'] = abs(eval_df['predicted'] - 215) / 215 * 100
    high_confidence = eval_df[eval_df['confidence'] > 5].copy()
    
    if len(high_confidence) > 0:
        correct_hc = sum((high_confidence['total_points'] > 215) == (high_confidence['predicted'] > 215))
        accuracy_hc = correct_hc / len(high_confidence) * 100
        roi_hc = (correct_hc - len(high_confidence)) / len(high_confidence) * 95  # -5% vig
        
        print(f"\n   å…¨éƒ¨æ¯”èµ› ({len(eval_df)}åœº):")
        print(f"   - Line 215 å‡†ç¡®ç‡: {accuracy_215:.1f}%")
        print(f"   - å¹³å‡è¯¯å·®: {eval_df['error'].abs().mean():.2f} åˆ†")
        
        print(f"\n   é«˜ä¿¡å¿ƒæ¯”èµ› (>{5}%, {len(high_confidence)}åœº):")
        print(f"   - Line 215 å‡†ç¡®ç‡: {accuracy_hc:.1f}%")
        print(f"   - ç†è®ºROI: {roi_hc:+.1f}%")
    else:
        print(f"\n   å…¨éƒ¨æ¯”èµ› ({len(eval_df)}åœº):")
        print(f"   - Line 215 å‡†ç¡®ç‡: {accuracy_215:.1f}%")
        print(f"   âš ï¸  æ— é«˜ä¿¡å¿ƒæ¯”èµ› (å…¨éƒ¨<5%)")
    
    return {
        'accuracy_215': accuracy_215,
        'avg_error': eval_df['error'].abs().mean(),
        'high_confidence_games': len(high_confidence),
        'high_confidence_accuracy': accuracy_hc if len(high_confidence) > 0 else 0,
        'roi': roi_hc if len(high_confidence) > 0 else 0
    }

def show_feature_importance(model, feature_cols):
    """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§"""
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§ Top 10:")
    
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:30s} {row['importance']:.1%}")
    
    # åˆ†ç»„ç»Ÿè®¡
    v2_importance = feature_importance[feature_importance['feature'].isin(feature_cols[:18])]['importance'].sum()
    v3_importance = feature_importance[feature_importance['feature'].isin(feature_cols[18:20])]['importance'].sum()
    v4_importance = feature_importance[feature_importance['feature'].isin(feature_cols[20:])]['importance'].sum()
    
    print(f"\n   ç‰¹å¾ç»„è´¡çŒ®:")
    print(f"   - V2åŸºç¡€ç‰¹å¾: {v2_importance:.1%}")
    print(f"   - V3ä¼¤ç—…ç‰¹å¾: {v3_importance:.1%}")
    print(f"   - ğŸ†• V4é˜²å®ˆèŠ‚å¥: {v4_importance:.1%}")

def save_model(model, filename='total_points_model_v4.pkl'):
    """ä¿å­˜æ¨¡å‹"""
    filepath = MODELS_DIR / filename
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
    print(f"   å¤§å°: {filepath.stat().st_size / 1024:.1f} KB")

def compare_with_v3():
    """åŠ è½½V3æ¨¡å‹å¯¹æ¯”"""
    v3_model_path = MODELS_DIR / 'total_points_model_v3.pkl'
    
    if not v3_model_path.exists():
        print(f"\nâš ï¸  V3æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ— æ³•å¯¹æ¯”")
        return
    
    print(f"\nğŸ“Š V3æ¨¡å‹åŸºçº¿æŒ‡æ ‡ (å‚è€ƒ):")
    print(f"   - MAE: 17.31 åˆ†")
    print(f"   - Line 215å‡†ç¡®ç‡: 73.5%")
    print(f"   - ç†è®ºROI: +40.3%")

def main():
    print("\n" + "="*70)
    print("ğŸ”§ è®­ç»ƒ NBA V4 æ¨¡å‹ (é˜²å®ˆæ•ˆç‡ + èŠ‚å¥)")
    print("="*70 + "\n")
    
    # åŠ è½½æ•°æ®
    df = load_features()
    X, y, metadata, feature_cols = prepare_data(df)
    
    # å¯¹æ¯”V3åŸºçº¿
    compare_with_v3()
    
    # è®­ç»ƒæ¨¡å‹
    model, cv_results, all_predictions = train_model(X, y, metadata)
    
    # è¯„ä¼°ç›˜å£å‡†ç¡®ç‡ï¼ˆç”¨CVçš„out-of-sampleé¢„æµ‹ï¼‰
    line_results = evaluate_line_accuracy(all_predictions)
    
    # ç‰¹å¾é‡è¦æ€§
    show_feature_importance(model, feature_cols)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model)
    
    # æœ€ç»ˆå¯¹æ¯”
    print(f"\n" + "="*70)
    print(f"ğŸ“Š V3 vs V4 å¯¹æ¯”:")
    print(f"="*70)
    print(f"\n{'æŒ‡æ ‡':<20s} {'V3 (20ç»´)':<15s} {'V4 (24ç»´)':<15s} {'æ”¹è¿›':<10s}")
    print(f"{'-'*70}")
    
    v4_mae_str = f"{cv_results['avg_mae']:.2f} åˆ†"
    v4_acc_str = f"{line_results['accuracy_215']:.1f}%"
    v4_roi_str = f"{line_results['roi']:+.1f}%"
    mae_diff = 17.31 - cv_results['avg_mae']
    acc_diff = line_results['accuracy_215'] - 73.5
    roi_diff = line_results['roi'] - 40.3
    
    print(f"{'MAE':<20s} {'17.31 åˆ†':<15s} {v4_mae_str:<15s} {mae_diff:+.2f} åˆ†")
    print(f"{'Line 215å‡†ç¡®ç‡':<20s} {'73.5%':<15s} {v4_acc_str:<15s} {acc_diff:+.1f}%")
    print(f"{'ç†è®ºROI':<20s} {'+40.3%':<15s} {v4_roi_str:<15s} {roi_diff:+.1f}%")
    
    print(f"\n" + "="*70)
    print(f"âœ… V4æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"="*70 + "\n")

if __name__ == '__main__':
    main()
