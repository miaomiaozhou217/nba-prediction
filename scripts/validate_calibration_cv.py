#!/usr/bin/env python3
"""
éªŒè¯æ ¡å‡†æ•ˆæœ (Time Series CV Out-of-Sample)
ç”¨çœŸæ­£çš„CVéªŒè¯é›†é¢„æµ‹è¯„ä¼°æ ¡å‡†æ•ˆæœ
"""
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
import xgboost as xgb

PROJECT_ROOT = Path(__file__).parent.parent
MODELS_DIR = PROJECT_ROOT / 'models'
DATA_DIR = PROJECT_ROOT / 'data'

def load_features():
    """åŠ è½½ç‰¹å¾æ•°æ®"""
    filepath = DATA_DIR / 'features' / 'features_v3.csv'
    return pd.read_csv(filepath)

def evaluate_predictions(predictions_df, calibration=0):
    """è¯„ä¼°é¢„æµ‹ï¼ˆåŸºäºout-of-sample CVé¢„æµ‹ï¼‰"""
    df = predictions_df.copy()
    
    # åº”ç”¨æ ¡å‡†
    if calibration != 0:
        df['predicted'] += calibration
    
    df['error'] = df['actual'] - df['predicted']
    
    # Line 215å‡†ç¡®ç‡
    correct_215 = sum((df['actual'] > 215) == (df['predicted'] > 215))
    accuracy_215 = correct_215 / len(df) * 100
    
    # é«˜ä¿¡å¿ƒä¸‹æ³¨ï¼ˆ>5%ï¼‰
    df['confidence'] = abs(df['predicted'] - 215) / 215 * 100
    high_conf = df[df['confidence'] > 5].copy()
    
    if len(high_conf) > 0:
        correct_hc = sum((high_conf['actual'] > 215) == (high_conf['predicted'] > 215))
        accuracy_hc = correct_hc / len(high_conf) * 100
        roi_hc = (correct_hc - len(high_conf)) / len(high_conf) * 95
    else:
        accuracy_hc = 0
        roi_hc = 0
    
    return {
        'mae': df['error'].abs().mean(),
        'avg_error': df['error'].mean(),
        'accuracy_215': accuracy_215,
        'high_conf_games': len(high_conf),
        'high_conf_accuracy': accuracy_hc,
        'roi': roi_hc
    }

def run_cv_with_predictions():
    """è¿è¡Œæ—¶é—´åºåˆ—CVï¼Œæ”¶é›†æ‰€æœ‰éªŒè¯é›†é¢„æµ‹"""
    print("ğŸ”§ è¿è¡Œ5æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯...\n")
    
    features_df = load_features()
    features_df = features_df.dropna(subset=['combined_pts_last_3', 'combined_pts_last_5'])
    
    # ç‰¹å¾åˆ—ï¼ˆV3: 20ç»´ï¼‰
    feature_cols = [
        'home_pts_last_3', 'home_pts_last_5', 'home_pts_last_10',
        'home_opp_pts_last_5', 'home_pts_std_5', 'home_pts_last_5_home',
        'away_pts_last_3', 'away_pts_last_5', 'away_pts_last_10',
        'away_opp_pts_last_5', 'away_pts_std_5', 'away_pts_last_5_away',
        'combined_pts_last_3', 'combined_pts_last_5', 'combined_pts_last_10',
        'home_off_vs_away_def', 'away_off_vs_home_def', 'home_field_advantage',
        'home_injury_impact', 'away_injury_impact'
    ]
    
    X = features_df[feature_cols].fillna(0)
    y = features_df['total_points']
    
    tscv = TimeSeriesSplit(n_splits=5)
    all_predictions = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = xgb.XGBRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            min_child_weight=3,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train, verbose=False)
        y_pred = model.predict(X_val)
        
        mae = mean_absolute_error(y_val, y_pred)
        print(f"   Fold {fold}: MAE={mae:.2f}, éªŒè¯é›†{len(val_idx)}åœº")
        
        # æ”¶é›†é¢„æµ‹
        for idx, pred in zip(val_idx, y_pred):
            all_predictions.append({
                'actual': y.iloc[idx],
                'predicted': pred
            })
    
    print(f"\nâœ… CVå®Œæˆï¼Œæ”¶é›†äº†{len(all_predictions)}åœºout-of-sampleé¢„æµ‹\n")
    
    return pd.DataFrame(all_predictions)

def main():
    print("\n" + "="*70)
    print("ğŸ“Š æ ¡å‡†éªŒè¯ (Out-of-Sample Time Series CV)")
    print("="*70 + "\n")
    
    # è¿è¡ŒCVè·å–çœŸå®é¢„æµ‹
    predictions_df = run_cv_with_predictions()
    
    # è¯„ä¼°åŸå§‹ç‰ˆ
    results_raw = evaluate_predictions(predictions_df, calibration=0)
    
    # è¯„ä¼°æ ¡å‡†ç‰ˆ
    results_cal = evaluate_predictions(predictions_df, calibration=2.7)
    
    # å¯¹æ¯”
    print("="*70)
    print(f"{'æŒ‡æ ‡':<25s} {'V3åŸå§‹':<15s} {'V3æ ¡å‡†(+2.7)':<15s} {'æ”¹è¿›':<10s}")
    print("-" * 70)
    
    metrics = [
        ('MAE', 'mae', 'åˆ†'),
        ('å¹³å‡åå·®', 'avg_error', 'åˆ†'),
        ('Line 215å‡†ç¡®ç‡', 'accuracy_215', '%'),
        ('é«˜ä¿¡å¿ƒæ¯”èµ›æ•°', 'high_conf_games', 'åœº'),
        ('é«˜ä¿¡å¿ƒå‡†ç¡®ç‡', 'high_conf_accuracy', '%'),
        ('ç†è®ºROI', 'roi', '%')
    ]
    
    for label, key, unit in metrics:
        raw_val = results_raw[key]
        cal_val = results_cal[key]
        diff = cal_val - raw_val
        
        if unit == 'åˆ†':
            raw_str = f"{raw_val:.2f}{unit}"
            cal_str = f"{cal_val:.2f}{unit}"
            diff_str = f"{diff:+.2f}{unit}"
        elif unit == '%':
            raw_str = f"{raw_val:.1f}{unit}"
            cal_str = f"{cal_val:.1f}{unit}"
            diff_str = f"{diff:+.1f}{unit}"
        else:
            raw_str = f"{int(raw_val)}{unit}"
            cal_str = f"{int(cal_val)}{unit}"
            diff_str = f"{int(diff):+d}{unit}"
        
        print(f"{label:<25s} {raw_str:<15s} {cal_str:<15s} {diff_str:<10s}")
    
    print("\n" + "="*70)
    print("ğŸ’¡ ç»“è®º:")
    print("-" * 70)
    
    if results_cal['mae'] < results_raw['mae']:
        print(f"âœ… æ ¡å‡†åMAEæ”¹å–„ {results_raw['mae'] - results_cal['mae']:.2f}åˆ†")
    else:
        print(f"âŒ æ ¡å‡†åMAEæ¶åŒ– {results_cal['mae'] - results_raw['mae']:.2f}åˆ†")
    
    if abs(results_cal['avg_error']) < abs(results_raw['avg_error']):
        print(f"âœ… ç³»ç»Ÿåå·®ä»{results_raw['avg_error']:.2f}åˆ†é™ä½åˆ°{results_cal['avg_error']:.2f}åˆ†")
    else:
        print(f"âš ï¸  ç³»ç»Ÿåå·®ä»{results_raw['avg_error']:.2f}åˆ†å˜ä¸º{results_cal['avg_error']:.2f}åˆ†")
    
    if results_cal['accuracy_215'] > results_raw['accuracy_215']:
        print(f"âœ… Line 215å‡†ç¡®ç‡æå‡ {results_cal['accuracy_215'] - results_raw['accuracy_215']:.1f}%")
    elif results_cal['accuracy_215'] < results_raw['accuracy_215']:
        print(f"âŒ Line 215å‡†ç¡®ç‡ä¸‹é™ {results_cal['accuracy_215'] - results_raw['accuracy_215']:.1f}%")
    else:
        print(f"â– Line 215å‡†ç¡®ç‡æ— å˜åŒ–")
    
    if results_cal['roi'] > results_raw['roi']:
        print(f"âœ… ç†è®ºROIæå‡ {results_cal['roi'] - results_raw['roi']:.1f}%")
    elif results_cal['roi'] < results_raw['roi']:
        print(f"âŒ ç†è®ºROIä¸‹é™ {results_cal['roi'] - results_raw['roi']:.1f}%")
    else:
        print(f"â– ç†è®ºROIæ— å˜åŒ–")
    
    # æœ€ç»ˆå»ºè®®
    print("\n" + "-" * 70)
    if (results_cal['mae'] < results_raw['mae'] and 
        results_cal['roi'] > results_raw['roi']):
        print("ğŸ† æ¨èä½¿ç”¨æ ¡å‡†ç‰ˆ (+2.7åˆ†)")
    elif (results_cal['mae'] > results_raw['mae'] or 
          results_cal['roi'] < results_raw['roi']):
        print("âŒ ä¸æ¨èæ ¡å‡†ï¼Œä¿æŒV3åŸå§‹ç‰ˆæœ¬")
    else:
        print("â– æ ¡å‡†æ•ˆæœä¸­æ€§ï¼Œç»§ç»­ç”¨V3åŸå§‹ç‰ˆè§‚å¯Ÿ")
    
    print("="*70 + "\n")

if __name__ == '__main__':
    main()
